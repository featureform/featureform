package provider

import (
	"bytes"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"github.com/featureform/metadata"
	"io"
	"math"
	"os"
	"os/exec"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	dp "github.com/novln/docker-parser"
	"go.uber.org/zap"
	_ "gocloud.dev/blob/fileblob"
	_ "gocloud.dev/blob/memblob"
	"golang.org/x/exp/slices"

	cfg "github.com/featureform/config"
	"github.com/featureform/filestore"
	"github.com/featureform/helpers"
	"github.com/featureform/kubernetes"
	"github.com/featureform/logging"
	pc "github.com/featureform/provider/provider_config"
)

const azureBlobStorePrefix = "abfss://"

// Hardcoded Go DateTime format, without milliseconds
// or UTC components, used for attempting to parse
// the timestamp column of an entity
const baseDateFormat = "2006-01-02 15:04:05"

type pandasOfflineQueries struct {
	defaultPythonOfflineQueries
}

func (q pandasOfflineQueries) trainingSetCreate(def TrainingSetDef, featureSchemas []ResourceSchema, labelSchema ResourceSchema) string {
	columns := make([]string, 0)
	joinQueries := make([]string, 0)
	featureTimestamps := make([]string, 0)
	for i, feature := range def.Features {
		featureColumnName := featureColumnName(feature)
		columns = append(columns, featureColumnName)
		var featureWindowQuery string
		// if no timestamp column, set to default generated by resource registration
		if featureSchemas[i].TS == "" {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, i+1, i+1, i+1)
		} else {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, featureSchemas[i].TS, i+1, i+1, i+1)
		}
		featureJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND t%d_ts <= label_ts)", featureWindowQuery, i+1, i+1, i+1)
		joinQueries = append(joinQueries, featureJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", i+1))
	}
	for i, lagFeature := range def.LagFeatures {
		lagFeaturesOffset := len(def.Features)
		idx := slices.IndexFunc(def.Features, func(id ResourceID) bool {
			return id.Name == lagFeature.FeatureName && id.Variant == lagFeature.FeatureVariant
		})
		lagSource := fmt.Sprintf("source_%d", idx)
		lagColumnName := sanitize(lagFeature.LagName)
		if lagFeature.LagName == "" {
			lagColumnName = sanitize(fmt.Sprintf("%s_%s_lag_%s", lagFeature.FeatureName, lagFeature.FeatureVariant, lagFeature.LagDelta))
		}
		columns = append(columns, lagColumnName)
		timeDeltaSeconds := lagFeature.LagDelta.Seconds() //parquet stores time as microseconds
		curIdx := lagFeaturesOffset + i + 1
		var lagWindowQuery string
		if featureSchemas[idx].TS == "" {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, curIdx, lagSource, curIdx)
		} else {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, featureSchemas[idx].TS, curIdx, lagSource, curIdx)
		}
		lagJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND DATETIME(t%d_ts, '+%f seconds') <= label_ts)", lagWindowQuery, curIdx, curIdx, curIdx, timeDeltaSeconds)
		joinQueries = append(joinQueries, lagJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", curIdx))
	}
	columnStr := strings.Join(columns, ", ")
	joinQueryString := strings.Join(joinQueries, " ")
	var labelWindowQuery string
	if labelSchema.TS == "" {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, 0 AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value)
	} else {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, %s AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value, labelSchema.TS)
	}
	labelPartitionQuery := fmt.Sprintf("(SELECT * FROM (SELECT entity, value, label_ts FROM (%s) t ) t0)", labelWindowQuery)
	labelJoinQuery := fmt.Sprintf("%s %s", labelPartitionQuery, joinQueryString)

	timeStamps := strings.Join(featureTimestamps, ", ")
	timeStampsDesc := strings.Join(featureTimestamps, " DESC,")
	fullQuery := fmt.Sprintf("SELECT %s, value AS %s, entity, label_ts, %s, ROW_NUMBER() over (PARTITION BY entity, value, label_ts ORDER BY label_ts DESC, %s DESC) as row_number FROM (%s) tt", columnStr, featureColumnName(def.Label), timeStamps, timeStampsDesc, labelJoinQuery)
	finalQuery := fmt.Sprintf("SELECT %s, %s FROM (SELECT * FROM (SELECT *, row_number FROM (%s) WHERE row_number=1 ))  ORDER BY label_ts", columnStr, featureColumnName(def.Label), fullQuery)
	return finalQuery
}

type K8sOfflineStore struct {
	executor Executor
	store    FileStore
	logger   *zap.SugaredLogger
	query    *pandasOfflineQueries
	BaseProvider
}

func (k8s *K8sOfflineStore) AsOfflineStore() (OfflineStore, error) {
	return k8s, nil
}

func (k8s *K8sOfflineStore) Close() error {
	return k8s.store.Close()
}

type Config []byte

type ExecutorFactory func(config Config, logger *zap.SugaredLogger) (Executor, error)

var executorFactoryMap = make(map[string]ExecutorFactory)

func RegisterExecutorFactory(name string, executorFactory ExecutorFactory) error {
	if _, exists := executorFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	executorFactoryMap[name] = executorFactory
	return nil
}

func CreateExecutor(name string, config Config, logger *zap.SugaredLogger) (Executor, error) {
	factory, exists := executorFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	executor, err := factory(config, logger)
	if err != nil {
		return nil, err
	}
	return executor, nil
}

type FileStoreFactory func(config Config) (FileStore, error)

var fileStoreFactoryMap = make(map[string]FileStoreFactory)

func RegisterFileStoreFactory(name string, FileStoreFactory FileStoreFactory) error {
	if _, exists := fileStoreFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	fileStoreFactoryMap[name] = FileStoreFactory
	return nil
}

func CreateFileStore(name string, config Config) (FileStore, error) {
	factory, exists := fileStoreFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	FileStore, err := factory(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create FileStore: %v", err)
	}
	return FileStore, nil
}

func init() {
	FileStoreFactoryMap := map[pc.FileStoreType]FileStoreFactory{
		pc.FileSystem: NewLocalFileStore,
		pc.Azure:      NewAzureFileStore,
		pc.S3:         NewS3FileStore,
		pc.GCS:        NewGCSFileStore,
		pc.HDFS:       NewHDFSFileStore,
	}
	executorFactoryMap := map[pc.ExecutorType]ExecutorFactory{
		pc.GoProc: NewLocalExecutor,
		pc.K8s:    NewKubernetesExecutor,
	}
	for storeType, factory := range FileStoreFactoryMap {
		err := RegisterFileStoreFactory(string(storeType), factory)
		if err != nil {
			panic(err)
		}
	}
	for executorType, factory := range executorFactoryMap {
		err := RegisterExecutorFactory(string(executorType), factory)
		if err != nil {
			panic(err)
		}
	}
}

func k8sOfflineStoreFactory(config pc.SerializedConfig) (Provider, error) {
	k8 := pc.K8sConfig{}
	logger := logging.NewLogger("kubernetes")
	if err := k8.Deserialize(config); err != nil {
		logger.Errorw("Invalid config to initialize k8s offline store", "error", err)
		return nil, fmt.Errorf("invalid k8s config: %w", err)
	}
	logger.Info("Creating executor with type:", k8.ExecutorType)
	execConfig := k8.ExecutorConfig.(pc.ExecutorConfig)
	serializedExecutor, err := execConfig.Serialize()
	if err != nil {
		logger.Errorw("Failure serializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}
	executor, err := CreateExecutor(string(k8.ExecutorType), serializedExecutor, logger)
	if err != nil {
		logger.Errorw("Failure initializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}

	serializedBlob, err := k8.StoreConfig.Serialize()
	if err != nil {
		return nil, fmt.Errorf("could not serialize blob store config")
	}

	logger.Info("Creating blob store with type:", k8.StoreType)
	store, err := CreateFileStore(string(k8.StoreType), serializedBlob)
	if err != nil {
		logger.Errorw("Failure initializing blob store with type", k8.StoreType, err)
		return nil, err
	}
	logger.Debugf("Store type: %s", k8.StoreType)
	queries := pandasOfflineQueries{}
	k8sOfflineStore := K8sOfflineStore{
		executor: executor,
		store:    store,
		logger:   logger,
		query:    &queries,
		BaseProvider: BaseProvider{
			ProviderType:   "K8S_OFFLINE",
			ProviderConfig: config,
		},
	}
	return &k8sOfflineStore, nil
}

type Executor interface {
	ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error
}

type LocalExecutor struct {
	scriptPath string
}

func (local LocalExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	envVars["MODE"] = "local"
	for key, value := range envVars {
		if err := os.Setenv(key, value); err != nil {
			return fmt.Errorf("could not set env variable: %s: %w", key, err)
		}
	}
	cmd := exec.Command("python3", local.scriptPath)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		return fmt.Errorf("could not execute python function: %v", err)
	}
	return nil
}

type LocalExecutorConfig struct {
	ScriptPath string
}

func (config *LocalExecutorConfig) Serialize() ([]byte, error) {
	data, err := json.Marshal(config)
	if err != nil {
		return nil, err
	}
	return data, nil
}

func (config *LocalExecutorConfig) Deserialize(data []byte) error {
	err := json.Unmarshal(data, config)
	if err != nil {
		return fmt.Errorf("deserialize executor config: %w", err)
	}
	return nil
}

func NewLocalExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	localConfig := LocalExecutorConfig{}
	if err := localConfig.Deserialize(config); err != nil {
		return nil, fmt.Errorf("failed to deserialize config")
	}
	_, err := os.Open(localConfig.ScriptPath)
	if err != nil {
		return nil, fmt.Errorf("could not find script path: %v", err)
	}
	return LocalExecutor{
		scriptPath: localConfig.ScriptPath,
	}, nil
}

type KubernetesExecutor struct {
	logger *zap.SugaredLogger
	image  string
}

// isDefaultImage checks that the current image name (excluding the tag) is the same as the default image
// name config.PandasBaseImage. It also validates that the name is a valid docker image name
func (kube *KubernetesExecutor) isDefaultImage() (bool, error) {
	parse, err := dp.Parse(kube.image)
	if err != nil {
		return false, fmt.Errorf("invalid image name: %w", err)
	}
	return parse.ShortName() == cfg.PandasBaseImage, nil
}

func (kube *KubernetesExecutor) setCustomImage(image string) {
	if image != "" {
		kube.image = image
	}
}

func (kube *KubernetesExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	kube.logger.Debugw("Executing k8s script", "args", args)
	var specs metadata.KubernetesResourceSpecs
	if args != nil {
		kube.setCustomImage(args.DockerImage)
		specs = args.Specs
	}
	if isDefault, err := kube.isDefaultImage(); err != nil {
		return fmt.Errorf("image check failed: %w", err)
	} else if !isDefault {
		kube.logger.Warnf("You are using a custom Docker Image (%s) for a Kubernetes job. This may have unintended behavior.", kube.image)
	}
	envVars["MODE"] = "k8s"
	resourceType, err := strconv.Atoi(envVars["RESOURCE_TYPE"])
	if err != nil {
		resourceType = 0
	}
	config := kubernetes.KubernetesRunnerConfig{
		JobPrefix: "kcf",
		EnvVars:   envVars,
		Image:     kube.image,
		NumTasks:  1,
		Resource: metadata.ResourceID{
			Name:    envVars["RESOURCE_NAME"],
			Variant: envVars["RESOURCE_VARIANT"],
			Type:    ProviderToMetadataResourceType[OfflineResourceType(resourceType)],
		},
		Specs: specs,
	}
	jobRunner, err := kubernetes.NewKubernetesRunner(config)
	if err != nil {
		return err
	}
	completionWatcher, err := jobRunner.Run()
	if err != nil {
		return err
	}
	if err := completionWatcher.Wait(); err != nil {
		return err
	}
	return nil
}

func NewKubernetesExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	var c pc.ExecutorConfig
	err := c.Deserialize(config)
	if err != nil {
		return nil, fmt.Errorf("could not create Kubernetes Executor: %w", err)
	}
	return &KubernetesExecutor{
		image:  c.GetImage(),
		logger: logger,
	}, nil
}

type FileStore interface {
	Write(key filestore.Filepath, data []byte) error
	Read(key filestore.Filepath) ([]byte, error)
	Serve(key filestore.Filepath) (Iterator, error)
	Exists(key filestore.Filepath) (bool, error)
	Delete(key filestore.Filepath) error
	DeleteAll(dir filestore.Filepath) error
	NewestFileOfType(prefix filestore.Filepath, fileType filestore.FileType) (filestore.Filepath, error)
	NumRows(key filestore.Filepath) (int64, error)
	Close() error
	Upload(sourcePath filestore.Filepath, destPath filestore.Filepath) error
	Download(sourcePath filestore.Filepath, destPath filestore.Filepath) error
	FilestoreType() pc.FileStoreType
	AddEnvVars(envVars map[string]string) map[string]string
}

type Iterator interface {
	Next() (map[string]interface{}, error)
	FeatureColumns() []string
	LabelColumn() string
}

func convertToParquetBytes(list []any) ([]byte, error) {
	// TODO possibly accepts single struct instead of list, have to be able to accept either, or another function
	if len(list) == 0 {
		return nil, fmt.Errorf("list is empty")
	}
	schema := parquet.SchemaOf(list[0])
	buf := new(bytes.Buffer)
	err := parquet.Write[any](
		buf,
		list,
		schema,
	)
	if err != nil {
		return nil, fmt.Errorf("could not write parquet file to bytes: %v", err)
	}
	return buf.Bytes(), nil
}

type ParquetIteratorMultipleFiles struct {
	fileList       []string
	currentFile    int64
	fileIterator   Iterator
	featureColumns []string
	labelColumn    string
	store          FileStore
}

func parquetIteratorOverMultipleFiles(fileParts []string, store FileStore) (Iterator, error) {
	b, err := store.Read(fileParts[0])
	//b, err := store.bucket.ReadAll(context.TODO(), fileParts[0])
	if err != nil {
		return nil, fmt.Errorf("could not read bucket: %w", err)
	}
	iterator, err := parquetIteratorFromBytes(b)
	if err != nil {
		return nil, fmt.Errorf("could not open first parquet file: %w", err)
	}
	return &ParquetIteratorMultipleFiles{
		fileList:     fileParts,
		currentFile:  int64(0),
		fileIterator: iterator,
		store:        store,
	}, nil
}

func (p *ParquetIteratorMultipleFiles) FeatureColumns() []string {
	return p.featureColumns
}

func (p *ParquetIteratorMultipleFiles) LabelColumn() string {
	return p.labelColumn
}

func (p *ParquetIteratorMultipleFiles) Next() (map[string]interface{}, error) {
	nextRow, err := p.fileIterator.Next()
	if err != nil {
		return nil, err
	}
	if nextRow == nil {
		if p.currentFile+1 == int64(len(p.fileList)) {
			return nil, nil
		}
		p.currentFile += 1
		b, err := p.store.Read(p.fileList[p.currentFile])
		if err != nil {
			return nil, err
		}
		iterator, err := parquetIteratorFromBytes(b)
		if err != nil {
			return nil, err
		}
		p.fileIterator = iterator
		return p.fileIterator.Next()
	}
	return nextRow, nil
}

type csvIterator struct {
	reader        *csv.Reader
	currentValues GenericRecord
	err           error
	columnNames   []string
	idx           int64
	limit         int64
}

func (c *csvIterator) Next() bool {
	if c.idx >= c.limit {
		return false
	}
	row, err := c.reader.Read()
	if err != nil {
		if err == io.EOF {
			return false
		} else {
			c.err = err
			return false
		}
	}
	c.currentValues = c.ParseRow(row)
	c.idx += 1
	return true
}

func (c *csvIterator) Values() GenericRecord {
	return c.currentValues
}

func (c *csvIterator) Columns() []string {
	return c.columnNames
}

func (c *csvIterator) Err() error {
	return c.err
}

func (c *csvIterator) Close() error {
	return nil
}

func (c *csvIterator) ParseRow(row []string) GenericRecord {
	records := make(GenericRecord, len(row))
	for i, value := range row {
		if integer, err := strconv.Atoi(value); err == nil {
			records[i] = integer
			continue
		}
		if float, err := strconv.ParseFloat(value, 64); err == nil {
			records[i] = float
			continue
		}
		records[i] = value
	}
	return records
}

func newCSVIterator(b []byte, limit int64) (GenericTableIterator, error) {
	reader := csv.NewReader(bytes.NewReader(b))
	headers, err := reader.Read()
	if err != nil {
		return nil, fmt.Errorf("failed to create CSV reader: %w", err)
	}
	if limit == -1 {
		limit = math.MaxInt64
	}
	return &csvIterator{
		reader:      reader,
		columnNames: headers,
		limit:       limit,
		idx:         0,
	}, nil
}

type parquetIterator struct {
	reader        *parquet.Reader
	currentValues GenericRecord
	err           error
	columnNames   []string
	limit         int64
	idx           int64
}

func (p *parquetIterator) Next() bool {
	if p.idx >= p.limit {
		return false
	}
	value := make(map[string]interface{})
	err := p.reader.Read(&value)
	if err != nil {
		if err == io.EOF {
			return false
		} else {
			p.err = err
			return false
		}
	}
	records := make(GenericRecord, 0)
	for idx := range p.columnNames {
		records = append(records, value[p.columnNames[idx]])
	}
	p.currentValues = records
	p.idx += 1
	return true
}

func (p *parquetIterator) Values() GenericRecord {
	return p.currentValues
}

func (p *parquetIterator) Columns() []string {
	return p.columnNames
}

func (p *parquetIterator) Err() error {
	return p.err
}

func (p *parquetIterator) Close() error {
	return p.reader.Close()
}

func newParquetIterator(b []byte, limit int64) (GenericTableIterator, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	columnList := r.Schema().Columns()
	columnNames := make([]string, len(columnList))
	for i, column := range columnList {
		columnNames[i] = column[0]
	}
	if limit == -1 {
		limit = math.MaxInt64
	}
	return &parquetIterator{
		reader:      r,
		columnNames: columnNames,
		limit:       limit,
		idx:         0,
	}, nil
}

type ParquetIterator struct {
	reader         *parquet.Reader
	index          int64
	featureColumns []string
	labelColumn    string
}

func (p *ParquetIterator) Next() (map[string]interface{}, error) {
	value := make(map[string]interface{})
	err := p.reader.Read(&value)
	if err != nil {
		if err == io.EOF {
			return nil, nil
		} else {
			return nil, err
		}
	}
	return value, nil
}

func (p *ParquetIterator) FeatureColumns() []string {
	return p.featureColumns
}

func (p *ParquetIterator) LabelColumn() string {
	return p.labelColumn
}

func getParquetNumRows(b []byte) (int64, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	return r.NumRows(), nil
}

type columnType string

const (
	labelType   columnType = "Label"
	featureType            = "Feature"
)

type parquetSchema struct {
	featureColumns []string
	labelColumn    string
}

func (s *parquetSchema) parseParquetColumnName(r *parquet.Reader) {
	columnList := r.Schema().Columns()
	for _, column := range columnList {
		columnName := column[0]
		colType := s.getColumnType(columnName)
		s.setColumn(colType, columnName)
	}
}
func (s *parquetSchema) getColumnType(name string) columnType {
	columnSections := strings.Split(name, "__")
	return columnType(columnSections[0])
}

func (s *parquetSchema) setColumn(colType columnType, name string) {
	if colType == labelType {
		s.labelColumn = name
	} else if colType == featureType {
		s.featureColumns = append(s.featureColumns, name)
	}
}

func parquetIteratorFromBytes(b []byte) (Iterator, error) {
	file := bytes.NewReader(b)
	r := parquet.NewReader(file)
	schema := parquetSchema{}
	schema.parseParquetColumnName(r)
	return &ParquetIterator{
		reader:         r,
		index:          int64(0),
		featureColumns: schema.featureColumns,
		labelColumn:    schema.labelColumn,
	}, nil
}

//Move this function to the resource ID
func ResourcePrefix(id ResourceID) string {
	return fmt.Sprintf("featureform/%s/%s/%s", id.Type, id.Name, id.Variant)
}

func fileStoreResourcePath(id ResourceID) string {
	return ResourcePrefix(id)
}

type BlobOfflineTable struct {
	schema ResourceSchema
}

func (tbl *BlobOfflineTable) Write(ResourceRecord) error {
	return fmt.Errorf("not yet implemented")
}

func (k8s *K8sOfflineStore) RegisterResourceFromSourceTable(id ResourceID, schema ResourceSchema) (OfflineTable, error) {
	return blobRegisterResource(id, schema, k8s.logger, k8s.store)
}

func blobRegisterResource(id ResourceID, schema ResourceSchema, logger *zap.SugaredLogger, store FileStore) (OfflineTable, error) {
	if err := id.check(Feature, Label); err != nil {
		logger.Errorw("Failure checking ID", "error", err)
		return nil, fmt.Errorf("ID check failed: %v", err)
	}
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	resourceExists, err := store.Exists(resourceKey)
	if err != nil {
		logger.Errorw("Error checking if resource exists", "error", err)
		return nil, fmt.Errorf("error checking if resource registry exists: %v", err)
	}
	if resourceExists {
		logger.Errorw("Resource already exists in blob store", "id", id, "ResourceKey", resourceKey)
		return nil, &TableAlreadyExists{id.Name, id.Variant}
	}
	serializedSchema, err := schema.Serialize()
	if err != nil {
		return nil, fmt.Errorf("error serializing resource schema: %s: %s", schema, err)
	}
	if err := store.Write(resourceKey, serializedSchema); err != nil {
		return nil, fmt.Errorf("error writing resource schema: %s: %s", schema, err)
	}
	logger.Debugw("Registered resource table", "resourceID", id, "for source", schema.SourceTable)
	return &BlobOfflineTable{schema}, nil
}

type FileStorePrimaryTable struct {
	store            FileStore
	source           Filepath
	isTransformation bool
	id               ResourceID
}

func (tbl *FileStorePrimaryTable) Write(GenericRecord) error {
	return fmt.Errorf("not implemented")
}

func (tbl *FileStorePrimaryTable) GetName() string {
	return tbl.source.FullPathWithBucket()
}

func (tbl *FileStorePrimaryTable) IterateSegment(n int64) (GenericTableIterator, error) {
	key := tbl.source.Path()
	keyParts := strings.Split(key, ".")
	// The length of keyParts is 1 if the key is a paht to a directory. This case is invalid
	// in the case of a primary table; however, we expect this case in the case of a transformation.
	if len(keyParts) == 1 {
		// The key should only be a directory in the case of transformations.
		if !tbl.isTransformation {
			return nil, fmt.Errorf("expected a file but got a directory: %s", keyParts[0])
		}
		// The file structure in cloud storage for transformations is /featureform/Transformation/<NAME>/<VARIANT>
		// but there is an additional directory that's named using a timestamp that contains the transformation file
		// we need to access. NewestFileOfType will recursively search for the newest file of the given type (i.e.
		// parquet) given a path (i.e. `key`).
		filename, err := tbl.store.NewestFileOfType(key, Parquet)
		if err != nil {
			return nil, fmt.Errorf("could not find newest file of type %s: %w", Parquet, err)
		}
		// We need to update key and keyParts with the result of NewestFileOfType for the Read below to succeed.
		key = filename
		keyParts = strings.Split(key, ".")
	}
	fmt.Printf("Reading file at key %s in file store type %s", key, tbl.store.FilestoreType())
	b, err := tbl.store.Read(key)
	if err != nil {
		return nil, fmt.Errorf("could not read file: %w", err)
	}
	switch fileType := keyParts[len(keyParts)-1]; fileType {
	case "parquet":
		return newParquetIterator(b, n)
	case "csv":
		return newCSVIterator(b, n)
	default:
		return nil, fmt.Errorf("unsupported file type: %s", fileType)
	}
}

func (tbl *FileStorePrimaryTable) NumRows() (int64, error) {
	return tbl.store.NumRows(tbl.source.Path())
}

type FileStoreIterator struct {
	iter    Iterator
	err     error
	curIdx  int64
	maxIdx  int64
	records []interface{}
	columns []string
}

func (it *FileStoreIterator) Next() bool {
	it.curIdx += 1
	if it.curIdx > it.maxIdx {
		return false
	}
	values, err := it.iter.Next()
	if values == nil {
		return false
	}
	if err != nil {
		it.err = err
		return false
	}
	records := make([]interface{}, 0)
	columns := make([]string, 0)
	for k, v := range values {
		columns = append(columns, k)
		records = append(records, v)
	}
	it.columns = columns
	it.records = records
	return true
}

func (it *FileStoreIterator) Columns() []string {
	return it.columns
}

func (it *FileStoreIterator) Err() error {
	return it.err
}

func (it *FileStoreIterator) Values() GenericRecord {
	return it.records
}

func (it *FileStoreIterator) Close() error {
	return nil
}

func (k8s *K8sOfflineStore) RegisterPrimaryFromSourceTable(id ResourceID, sourcePath string) (PrimaryTable, error) {
	return blobRegisterPrimary(id, sourcePath, k8s.logger, k8s.store)
}

func blobRegisterPrimary(id ResourceID, sourcePath string, logger *zap.SugaredLogger, store FileStore) (PrimaryTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Infow("Checking if resource key exists", "key", resourceKey)
	primaryExists, err := store.Exists(resourceKey)
	if err != nil {
		logger.Errorw("Error checking if primary exists", "error", err)
		return nil, fmt.Errorf("error checking if primary exists: %v", err)
	}
	if primaryExists {
		logger.Errorw("Primary table already exists", "source", sourcePath)
		return nil, fmt.Errorf("primary already exists")
	}

	logger.Debugw("Registering primary table", "id", id, "source", sourcePath)
	// **NOTE:** The data we're writing to the blob store is the path to the primary source data file.
	// This blob will be read by other processes (e.g. transformation jobs) to fetch where the primary
	// data is stored prior to acting on it. You can verify this by accessing the object stored at
	// /featureform/Primary/<NAME>/<VARIANT>
	if err := store.Write(resourceKey, []byte(sourcePath)); err != nil {
		logger.Errorw("Could not write primary table", "error", err)
		return nil, err
	}

	filePath, err := NewEmptyFilepath(store.FilestoreType())
	if err != nil {
		logger.Errorw("Could not create empty filepath", "error", err, "storeType", store.FilestoreType(), "sourcePath", sourcePath)
		return nil, err
	}
	err = filePath.ParseFullPath(sourcePath)
	if err != nil {
		logger.Errorw("Could not parse full path", "error", err, "sourcePath", sourcePath)
		return nil, err
	}
	logger.Debugw("Successfully registered primary table", "id", id, "source", sourcePath)
	return &FileStorePrimaryTable{store, filePath, false, id}, nil
}

func (k8s *K8sOfflineStore) CreateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, false)
}

func (k8s *K8sOfflineStore) transformation(config TransformationConfig, isUpdate bool) error {
	if config.Type == SQLTransformation {
		return k8s.sqlTransformation(config, isUpdate)
	} else if config.Type == DFTransformation {
		return k8s.dfTransformation(config, isUpdate)
	} else {
		k8s.logger.Errorw("the transformation type is not supported", "type", config.Type)
		return fmt.Errorf("the transformation type '%v' is not supported", config.Type)
	}
}

func addETCDVars(envVars map[string]string) map[string]string {
	etcdHost := helpers.GetEnv("ETCD_HOST", "localhost")
	etcdPort := helpers.GetEnv("ETCD_PORT", "2379")
	etcdPassword := helpers.GetEnv("ETCD_PASSWORD", "secretpassword")
	etcdUsername := helpers.GetEnv("ETCD_USERNAME", "root")
	envVars["ETCD_HOST"] = etcdHost
	envVars["ETCD_PASSWORD"] = etcdPassword
	envVars["ETCD_PORT"] = etcdPort
	envVars["ETCD_USERNAME"] = etcdUsername
	return envVars
}

func (k8s *K8sOfflineStore) pandasRunnerArgs(outputURI string, updatedQuery string, sources []string, jobType JobType) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "sql",
		"TRANSFORMATION":      updatedQuery,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func (k8s K8sOfflineStore) getDFArgs(outputURI string, code string, mapping []SourceMapping, sources []string) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "df",
		"TRANSFORMATION":      code,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func addResourceID(envVars map[string]string, id ResourceID) map[string]string {
	envVars["RESOURCE_NAME"] = id.Name
	envVars["RESOURCE_VARIANT"] = id.Variant
	envVars["RESOURCE_TYPE"] = fmt.Sprintf("%d", id.Type)
	return envVars
}

func (k8s *K8sOfflineStore) sqlTransformation(config TransformationConfig, isUpdate bool) error {
	updatedQuery, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		k8s.logger.Errorw("Could not generate updated query for k8s transformation", err)
		return err
	}

	transformationDestination := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	transformationDestinationExactPath, err := k8s.store.NewestFileOfType(transformationDestination, Parquet)
	if err != nil {
		k8s.logger.Errorw("Could not get newest blob", "location", transformationDestination, "error", err)
		return fmt.Errorf("could not get newest blob: %s: %v", transformationDestination, err)
	}
	exists := transformationDestinationExactPath != ""
	if !isUpdate && exists {
		k8s.logger.Errorw("Creation when transformation already exists", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, transformationDestination)
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Update job attempted when transformation does not exist", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, transformationDestination)
	}
	k8s.logger.Debugw("Running SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	runnerArgs := k8s.pandasRunnerArgs(transformationDestination, updatedQuery, sources, Transform)
	runnerArgs = addResourceID(runnerArgs, config.TargetTableID)

	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(runnerArgs, &args); err != nil {
		k8s.logger.Errorw("job for transformation failed to run", "target_table", config.TargetTableID, "error", err)
		return fmt.Errorf("job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	return nil
}

func (k8s *K8sOfflineStore) checkArgs(args metadata.TransformationArgs) (metadata.KubernetesArgs, error) {
	k8sArgs, ok := args.(metadata.KubernetesArgs)
	if !ok {
		return metadata.KubernetesArgs{}, fmt.Errorf("invalid type used for Kubernetes Arguments")
	}
	return k8sArgs, nil
}

func (k8s *K8sOfflineStore) dfTransformation(config TransformationConfig, isUpdate bool) error {
	_, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		return err
	}
	transformationDestination := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	exists, err := k8s.store.Exists(transformationDestination)
	if err != nil {
		k8s.logger.Errorw("Error checking if resource exists", "error", err)
		return err
	}

	if !isUpdate && exists {
		k8s.logger.Errorw("Transformation already exists", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, transformationDestination)
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Transformation doesn't exists at destination and you are trying to update", "target_table", config.TargetTableID, "destination", transformationDestination)
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, transformationDestination)
	}

	transformationFilePath := k8s.store.PathWithPrefix(fileStoreResourcePath(config.TargetTableID), false)
	fileName := "transformation.pkl"
	transformationFileLocation := fmt.Sprintf("%s%s", transformationFilePath, fileName)
	err = k8s.store.Write(transformationFileLocation, config.Code)
	if err != nil {
		return fmt.Errorf("could not upload file: %v", err)
	}

	dfArgs := k8s.getDFArgs(transformationDestination, transformationFileLocation, config.SourceMapping, sources)
	dfArgs = addResourceID(dfArgs, config.TargetTableID)
	k8s.logger.Debugw("Running DF transformation", "target_table", config.TargetTableID)
	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(dfArgs, &args); err != nil {
		k8s.logger.Errorw("Error running dataframe job", "error", err)
		return fmt.Errorf("submit job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran DF transformation", "target_table", config.TargetTableID)
	return nil
}

func (k8s *K8sOfflineStore) updateQuery(query string, mapping []SourceMapping) (string, []string, error) {
	sources := make([]string, len(mapping))
	replacements := make([]string, len(mapping)*2) // It's times 2 because each replacement will be a pair; (original, replacedValue)

	for i, m := range mapping {
		replacements = append(replacements, m.Template)
		replacements = append(replacements, fmt.Sprintf("source_%v", i))

		sourcePath := ""
		sourcePath, err := k8s.getSourcePath(m.Source)
		k8s.logger.Debugw("Fetched Source Path", "source", m.Source, "sourcePath", sourcePath)
		if err != nil {
			k8s.logger.Errorw("Error getting source path of source", "source", m.Source, "error", err)
			return "", nil, fmt.Errorf("could not get the sourcePath for %s because %s", m.Source, err)
		}

		sources[i] = sourcePath
	}

	replacer := strings.NewReplacer(replacements...)
	updatedQuery := replacer.Replace(query)

	if strings.Contains(updatedQuery, "{{") {
		k8s.logger.Errorw("Template replace failed", "query", updatedQuery)
		return "", nil, fmt.Errorf("could not replace all the templates with the current mapping. Mapping: %v; Replaced Query: %s", mapping, updatedQuery)
	}
	return updatedQuery, sources, nil
}

func (k8s *K8sOfflineStore) getSourcePath(path string) (string, error) {
	fileType, fileName, fileVariant := k8s.getResourceInformationFromFilePath(path)
	k8s.logger.Debugw("Retrieved source path", "fileType", fileType, "fileName", fileName, "fileVariant", fileVariant)

	var filePath string
	if fileType == "primary" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Primary}
		fileTable, err := k8s.GetPrimaryTable(fileResourceId)
		if err != nil {
			k8s.logger.Errorw("Issue getting primary table", "id", fileResourceId, "error", err)
			return "", fmt.Errorf("could not get the primary table for {%v} because %s", fileResourceId, err)
		}
		filePath = fileTable.GetName()
		return filePath, nil
	} else if fileType == "transformation" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Transformation}
		fileResourcePath := k8s.store.PathWithPrefix(fileStoreResourcePath(fileResourceId), false)
		k8s.logger.Debugw("Retrieved transformation source", "ResourceId", fileResourceId, "fileResourcePath", fileResourcePath)
		// get file type of source
		sourceFileExtension := FileType(filepath.Ext(fileResourcePath))
		exactFileResourcePath, err := k8s.store.NewestFileOfType(fileResourcePath, sourceFileExtension)
		k8s.logger.Debugw("Retrieved latest file path", "exactFileResourcePath", exactFileResourcePath)
		if err != nil {
			k8s.logger.Errorw("Could not get newest blob", "location", fileResourcePath, "error", err)
			return "", fmt.Errorf("could not get newest blob: %s: %v", fileResourcePath, err)
		}
		if exactFileResourcePath == "" {
			k8s.logger.Errorw("Issue getting transformation table", "id", fileResourceId)
			return "", fmt.Errorf("could not get the transformation table for {%v} at {%s}", fileResourceId, fileResourcePath)
		}
		filePath := k8s.store.PathWithPrefix(exactFileResourcePath[:strings.LastIndex(exactFileResourcePath, "/")+1], false)
		return filePath, nil
	} else {
		return filePath, fmt.Errorf("could not find path for %s; fileType: %s, fileName: %s, fileVariant: %s", path, fileType, fileName, fileVariant)
	}
}

func (k8s *K8sOfflineStore) getResourceInformationFromFilePath(path string) (string, string, string) {
	var fileType string
	var fileName string
	var fileVariant string
	if path[:5] == "s3://" {
		filePaths := strings.Split(path[len("s3://"):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else if path[:5] == HDFSPrefix {
		filePaths := strings.Split(path[len(HDFSPrefix):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else {
		filePaths := strings.Split(path[len("featureform_"):], "__")
		if len(filePaths) <= 2 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = filePaths[0], filePaths[1], filePaths[2]
	}
	return fileType, fileName, fileVariant
}

func (k8s *K8sOfflineStore) GetTransformationTable(id ResourceID) (TransformationTable, error) {
	transformationPath := k8s.store.PathWithPrefix(fileStoreResourcePath(id), true)
	k8s.logger.Debugw("Retrieved transformation source", "ResourceId", id, "transformationPath", transformationPath)
	filePath, err := NewEmptyFilepath(k8s.store.FilestoreType())
	if err != nil {
		k8s.logger.Errorw("Could not create empty filepath", "error", err, "storeType", k8s.store.FilestoreType(), "transformationPath", transformationPath)
		return nil, err
	}
	err = filePath.ParseFullPath(transformationPath)
	if err != nil {
		k8s.logger.Errorw("Could not parse full path", "error", err, "transformationPath", transformationPath)
		return nil, err
	}
	if err != nil {
		k8s.logger.Errorw("Could not create empty filepath", "error", err, "storeType", k8s.store.FilestoreType(), "transformationPath", transformationPath)
		return nil, err
	}
	return &FileStorePrimaryTable{k8s.store, filePath, true, id}, nil
}

func (k8s *K8sOfflineStore) UpdateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, true)
}
func (k8s *K8sOfflineStore) CreatePrimaryTable(id ResourceID, schema TableSchema) (PrimaryTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetPrimaryTable(id ResourceID) (PrimaryTable, error) {
	return fileStoreGetPrimary(id, k8s.store, k8s.logger)
}

func fileStoreGetPrimary(id ResourceID, store FileStore, logger *zap.SugaredLogger) (PrimaryTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Debugw("Getting primary table", "id", id, "resourceKey", resourceKey)
	table, err := store.Read(resourceKey)
	logger.Debugw("Read primary table", "table", string(table), "error", err)
	if err != nil {
		return nil, fmt.Errorf("error fetching primary table: %v", err)
	}
	filePath, err := NewEmptyFilepath(store.FilestoreType())
	if err != nil {
		logger.Errorw("Could not create empty filepath", "error", err, "storeType", store.FilestoreType(), "resourceKey", resourceKey)
		return nil, err
	}
	err = filePath.ParseFullPath(string(table))
	if err != nil {
		logger.Errorw("Could not parse full path", "error", err, "table", string(table))
		return nil, err
	}
	logger.Debugw("Successfully retrieved primary table", "id", id)
	return &FileStorePrimaryTable{store, filePath, false, id}, nil
}

func (k8s *K8sOfflineStore) CreateResourceTable(id ResourceID, schema TableSchema) (OfflineTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetResourceTable(id ResourceID) (OfflineTable, error) {
	return fileStoreGetResourceTable(id, k8s.store, k8s.logger)
}

func fileStoreGetResourceTable(id ResourceID, store FileStore, logger *zap.SugaredLogger) (OfflineTable, error) {
	resourceKey := store.PathWithPrefix(fileStoreResourcePath(id), false)
	logger.Debugw("Getting resource table", "id", id, "resourceKey", resourceKey)
	serializedSchema, err := store.Read(resourceKey)
	if err != nil {
		return nil, fmt.Errorf("error reading schema bytes from blob storage: %v", err)
	}
	resourceSchema := ResourceSchema{}
	if err := resourceSchema.Deserialize(serializedSchema); err != nil {
		return nil, fmt.Errorf("error deserializing resource table: %v", err)
	}
	logger.Debugw("Successfully fetched resource table", "id", id)
	return &BlobOfflineTable{resourceSchema}, nil
}

func (k8s *K8sOfflineStore) CreateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, false)
}

func (k8s *K8sOfflineStore) GetMaterialization(id MaterializationID) (Materialization, error) {
	return fileStoreGetMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreGetMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) (Materialization, error) {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization", "id", id)
		return nil, fmt.Errorf("invalid materialization id: %v", id)
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	logger.Debugw("Getting materialization", "id", id)
	materializationPath := store.PathWithPrefix(fileStoreResourcePath(materializationID), false)
	materializationExactPath, err := store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		logger.Errorw("Could not fetch materialization resource key", "error", err)
		return nil, fmt.Errorf("could not fetch materialization resource key: %v", err)
	}
	logger.Debugw("Successfully retrieved materialization", "id", id)
	return &FileStoreMaterialization{materializationID, store, materializationExactPath}, nil
}

type FileStoreMaterialization struct {
	id    ResourceID
	store FileStore
	key   string
}

func (mat FileStoreMaterialization) ID() MaterializationID {
	return MaterializationID(fmt.Sprintf("%s/%s/%s", FeatureMaterialization, mat.id.Name, mat.id.Variant))
}

func (mat FileStoreMaterialization) NumRows() (int64, error) {
	materializationPath := mat.store.PathWithPrefix(fileStoreResourcePath(mat.id), false)
	latestMaterializationPath, err := mat.store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return 0, fmt.Errorf("could not get materialization num rows; %v", err)
	}
	return mat.store.NumRows(latestMaterializationPath)
}

func (mat FileStoreMaterialization) IterateSegment(begin, end int64) (FeatureIterator, error) {
	materializationPath := mat.store.PathWithPrefix(fileStoreResourcePath(mat.id), false)
	latestMaterializationPath, err := mat.store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get materialization iterate segment: %v", err)
	}
	iter, err := mat.store.Serve(latestMaterializationPath)
	if err != nil {
		return nil, err
	}
	for i := int64(0); i < begin; i++ {
		_, _ = iter.Next()
	}
	return &FileStoreFeatureIterator{
		iter:   iter,
		curIdx: 0,
		maxIdx: end,
	}, nil
}

type FileStoreFeatureIterator struct {
	iter   Iterator
	err    error
	cur    ResourceRecord
	curIdx int64
	maxIdx int64
}

func (iter *FileStoreFeatureIterator) Next() bool {
	iter.curIdx += 1
	if iter.curIdx > iter.maxIdx {
		return false
	}
	nextVal, err := iter.iter.Next()
	if err != nil {
		iter.err = err
		return false
	}
	if nextVal == nil {
		return false
	}
	value, err := iter.parseValue(nextVal["value"])
	if err != nil {
		iter.err = err
		return false
	}
	ts, hasTimestamp := nextVal["ts"].(string)
	var timestamp time.Time
	if hasTimestamp {
		timestamp, err = iter.parseTimestamp(ts)
		if err != nil {
			iter.err = err
			return false
		}
	}
	iter.cur = ResourceRecord{
		Entity: nextVal["entity"].(string),
		Value:  value,
		TS:     timestamp,
	}
	return true
}

// Attempts to parse timestamp in one of the following formats:
// 1. "2006-01-02 15:04:05.000000 UTC"
// 2. "2006-01-02 15:04:05.000000"
// 3. "2006-01-02 15:04:05.000000 +0000 UTC"
// If any one of the three formats is valid, returns the parsed timestamp, otherwise it
// returns an error
func (iter *FileStoreFeatureIterator) parseTimestamp(ts string) (time.Time, error) {
	formats := []string{
		fmt.Sprintf("%s UTC", baseDateFormat),
		baseDateFormat,
		fmt.Sprintf("%s +0000 UTC", baseDateFormat),
	}
	for _, format := range formats {
		timestamp, err := time.Parse(format, ts)
		if err == nil {
			return timestamp, nil
		}
	}
	return time.Time{}, fmt.Errorf("could not parse timestamp: %v", ts)
}

// Attempts to parse value in one of the following formats:
// 1. a scalar value (string, int, float, bool)
// 2. []float32 (i.e. vector32)
func (iter *FileStoreFeatureIterator) parseValue(value interface{}) (interface{}, error) {
	valueMap, ok := value.(map[string]interface{})
	if !ok {
		return value, nil
	}
	list, ok := valueMap["list"]
	if !ok {
		return "", fmt.Errorf("expected to find field 'list' value (type %T)", value)
	}
	// To iterate over the list and create a we need to cast it to []interface{}
	elementsSlice, ok := list.([]interface{})
	if !ok {
		return "", fmt.Errorf("could not cast type: %T to []interface{}", list)
	}
	vector32 := make([]float32, len(elementsSlice))
	for i, e := range elementsSlice {
		// To access the 'element' field, which holds the float value,
		// we need to cast it to map[string]interface{}
		m, ok := e.(map[string]interface{})
		if !ok {
			return "", fmt.Errorf("could not cast type: %T to map[string]interface{}", e)
		}
		switch element := m["element"].(type) {
		case float32:
			vector32[i] = element
		// Given floats in Python are typically 64-bit, it's possible we'll receive
		// a vector of float64
		case float64:
			vector32[i] = float32(element)
		default:
			return "", fmt.Errorf("unexpected type in parquet vector list: %T", element)
		}
	}
	return vector32, nil
}

func (iter *FileStoreFeatureIterator) Value() ResourceRecord {
	return iter.cur
}

func (iter *FileStoreFeatureIterator) Err() error {
	return iter.err
}

func (iter *FileStoreFeatureIterator) Close() error {
	return nil
}

func (k8s *K8sOfflineStore) UpdateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, true)
}

func (k8s *K8sOfflineStore) materialization(id ResourceID, isUpdate bool) (Materialization, error) {
	if id.Type != Feature {
		k8s.logger.Errorw("Attempted to create a materialization of a non feature resource", "type", id.Type)
		return nil, fmt.Errorf("only features can be materialized")
	}
	resourceTable, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Attempted to fetch resource table of non registered resource", "error", err)
		return nil, fmt.Errorf("resource not registered: %v", err)
	}
	k8sResourceTable, ok := resourceTable.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("Could not convert resource table to blob offline table", "id", id)
		return nil, fmt.Errorf("could not convert offline table with id %v to k8sResourceTable", id)
	}
	materializationID := ResourceID{Name: id.Name, Variant: id.Variant, Type: FeatureMaterialization}
	featureResourcePath := fileStoreResourcePath(materializationID)
	destinationPath := k8s.store.PathWithPrefix(featureResourcePath, false)
	materializationNewestFile, err := k8s.store.NewestFileOfType(destinationPath, Parquet)
	k8s.logger.Debugw("Running Materialization", "id", id, "destinationPath", destinationPath, "materializationNewestFile", materializationNewestFile)
	materializationExists := materializationNewestFile != ""
	if err != nil {
		k8s.logger.Errorw("Could not determine whether materialization exists", err)
		return nil, fmt.Errorf("error checking if materialization exists: %v", err)
	}
	if !isUpdate && materializationExists {
		k8s.logger.Errorw("Attempted to materialize a materialization that already exists", "id", id)
		return nil, fmt.Errorf("materialization already exists")
	} else if isUpdate && !materializationExists {
		k8s.logger.Errorw("Attempted to update a materialization that does not exist", "id", id)
		return nil, fmt.Errorf("materialization does not exist")
	}
	materializationQuery := k8s.query.materializationCreate(k8sResourceTable.schema)
	sourcePath := k8s.store.PathWithPrefix(k8sResourceTable.schema.SourceTable, false)
	// get source path file type; note, it's possible it doesn't have it
	fileType := GetFileType(sourcePath)
	newestSourcePath, err := k8s.store.NewestFileOfType(sourcePath, fileType)
	k8s.logger.Debugw("Retrieved newest source path", "sourcePath", sourcePath, "newestSourcePath", newestSourcePath)
	if err != nil {
		k8s.logger.Errorw("Could not determine newest source file for materialization", "sourcePath", sourcePath, "error", err)
		return nil, fmt.Errorf("error determining newest source file: %v", err)
	}
	k8sArgs := k8s.pandasRunnerArgs(destinationPath, materializationQuery, []string{newestSourcePath}, Materialize)
	k8sArgs = addResourceID(k8sArgs, id)
	if err := k8s.executor.ExecuteScript(k8sArgs, nil); err != nil {
		k8s.logger.Errorw("Job failed to run", "error", err)
		return nil, fmt.Errorf("job for materialization %v failed to run: %v", materializationID, err)
	}

	k8s.logger.Debugw("Successfully created materialization", "id", id)
	return &FileStoreMaterialization{materializationID, k8s.store, materializationNewestFile}, nil
}

func (k8s *K8sOfflineStore) DeleteMaterialization(id MaterializationID) error {
	return fileStoreDeleteMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreDeleteMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) error {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization id", id)
		return fmt.Errorf("invalid materialization id")
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	materializationPath := store.PathWithPrefix(fileStoreResourcePath(materializationID), false)
	materializationExactPath, err := store.NewestFileOfType(materializationPath, Parquet)
	if err != nil {
		return fmt.Errorf("materialization does not exist: %v", err)
	}
	return store.Delete(materializationExactPath)
}

func (k8s *K8sOfflineStore) CreateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, false)
}

func (k8s *K8sOfflineStore) UpdateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, true)
}

func (k8s *K8sOfflineStore) registeredResourceSchema(id ResourceID) (ResourceSchema, error) {
	k8s.logger.Debugw("Getting resource schema", "id", id)
	table, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Resource not registered in blob store", "id", id, "error", err)
		return ResourceSchema{}, fmt.Errorf("resource not registered: %v", err)
	}
	blobResourceTable, ok := table.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("could not convert offline table to blobResourceTable", "id", id)
		return ResourceSchema{}, fmt.Errorf("could not convert offline table with id %v to blobResourceTable", id)
	}
	k8s.logger.Debugw("Successfully retrieved resource schema", "id", id, "schema", blobResourceTable.schema)
	return blobResourceTable.schema, nil
}

func (k8s *K8sOfflineStore) trainingSet(def TrainingSetDef, isUpdate bool) error {
	if err := def.check(); err != nil {
		k8s.logger.Errorw("Training set definition not valid", def, err)
		return err
	}
	sourcePaths := make([]string, 0)
	featureSchemas := make([]ResourceSchema, 0)
	destinationPath := k8s.store.PathWithPrefix(fileStoreResourcePath(def.ID), false)
	trainingSetExactPath, err := k8s.store.NewestFileOfType(destinationPath, Parquet)

	k8s.logger.Debugw("Running Training Set", "id", def.ID, "destinationPath", destinationPath, "trainingSetExactPath", trainingSetExactPath)

	if err != nil {
		return fmt.Errorf("could not get training set path: %v", err)
	}
	trainingSetExists := !(trainingSetExactPath == "")
	if !isUpdate && trainingSetExists {
		k8s.logger.Errorw("Training set already exists", "id", def.ID)
		return fmt.Errorf("k8s training set already exists: %v", def.ID)
	} else if isUpdate && !trainingSetExists {
		k8s.logger.Errorw("Training set doesn't exist for update job", def.ID)
		return fmt.Errorf("training set doesn't exist for update job: %v", def.ID)
	}
	labelSchema, err := k8s.registeredResourceSchema(def.Label)
	if err != nil {
		k8s.logger.Errorw("Could not get schema of label in store", "id", def.Label, "error", err)
		return fmt.Errorf("could not get schema of label %s: %v", def.Label, err)
	}
	labelPath := labelSchema.SourceTable
	fileType := GetFileType(labelPath)
	latestLabelFile, err := k8s.store.NewestFileOfType(labelPath, fileType)
	k8s.logger.Debugw("Latest label file", "labelPath", labelPath, "latestLabelFile", latestLabelFile)
	if err != nil {
		k8s.logger.Errorw("Could not get latest label file", "error", err)
		return fmt.Errorf("could not get latest label file: %v", err)
	}
	sourcePaths = append(sourcePaths, latestLabelFile)
	for _, feature := range def.Features {
		featureSchema, err := k8s.registeredResourceSchema(feature)
		if err != nil {
			k8s.logger.Errorw("Could not get schema of feature in store", "feature", feature, "error", err)
			return fmt.Errorf("could not get schema of feature %s: %v", feature, err)
		}
		featurePath := featureSchema.SourceTable
		fileType := GetFileType(featurePath)
		latestFeatureFile, err := k8s.store.NewestFileOfType(featurePath, fileType)
		k8s.logger.Debugw("Latest feature file", "featurePath", featurePath, "latestFeatureFile", latestFeatureFile)
		if err != nil {
			k8s.logger.Errorw("Could not get latest feature file", "error", err)
			return fmt.Errorf("could not get latest feature file: %v", err)
		}
		sourcePaths = append(sourcePaths, latestFeatureFile)
		featureSchemas = append(featureSchemas, featureSchema)
	}
	trainingSetQuery := k8s.query.trainingSetCreate(def, featureSchemas, labelSchema)
	k8s.logger.Debugw("Source List", "SourceFiles", sourcePaths)
	k8s.logger.Debugw("Training Set Query", "list", trainingSetQuery)
	pandasArgs := k8s.pandasRunnerArgs(k8s.store.PathWithPrefix(destinationPath, false), trainingSetQuery, sourcePaths, CreateTrainingSet)
	pandasArgs = addResourceID(pandasArgs, def.ID)
	k8s.logger.Debugw("Creating training set", "definition", def)

	if err := k8s.executor.ExecuteScript(pandasArgs, nil); err != nil {
		k8s.logger.Errorw("training set job failed to run", "definition", def.ID, "error", err)
		return fmt.Errorf("job for training set %v failed to run: %v", def.ID, err)
	}
	k8s.logger.Debugw("Successfully created training set:", "definition", def)
	return nil
}

func (k8s *K8sOfflineStore) GetTrainingSet(id ResourceID) (TrainingSetIterator, error) {
	return fileStoreGetTrainingSet(id, k8s.store, k8s.logger)
}

func fileStoreGetTrainingSet(id ResourceID, store FileStore, logger *zap.SugaredLogger) (TrainingSetIterator, error) {
	if err := id.check(TrainingSet); err != nil {
		logger.Errorw("Resource is not of type training set", "error", err)
		return nil, fmt.Errorf("resource is not training set: %w", err)
	}
	resourceKeyPrefix := store.PathWithPrefix(fileStoreResourcePath(id), false)
	trainingSetExactPath, err := store.NewestFileOfType(resourceKeyPrefix, Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get training set: %v", err)
	}
	if trainingSetExactPath == "" {
		return nil, fmt.Errorf("the training set (%v at resource prefix: %s) does not exist", id, resourceKeyPrefix)
	}

	iterator, err := store.Serve(trainingSetExactPath)
	if err != nil {
		return nil, fmt.Errorf("could not serve training set: %w", err)
	}
	return &FileStoreTrainingSet{id: id, store: store, key: trainingSetExactPath, iter: iterator}, nil
}

type FileStoreTrainingSet struct {
	id       ResourceID
	store    FileStore
	key      string
	iter     Iterator
	Error    error
	features []interface{}
	label    interface{}
}

func (ts *FileStoreTrainingSet) Next() bool {
	row, err := ts.iter.Next()
	if err != nil {
		ts.Error = err
		return false
	}
	if row == nil {
		return false
	}
	featureValues := make([]interface{}, len(ts.iter.FeatureColumns()))
	for i, key := range ts.iter.FeatureColumns() {
		featureValues[i] = row[key]
	}
	ts.features = featureValues
	ts.label = row[ts.iter.LabelColumn()]
	return true
}

func (ts *FileStoreTrainingSet) Features() []interface{} {
	return ts.features
}

func (ts *FileStoreTrainingSet) Label() interface{} {
	return ts.label
}

func (ts *FileStoreTrainingSet) Err() error {
	return ts.Error
}
